{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d51501f2-faaf-49cd-8be8-dc915fc48388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### https://docs.ray.io/en/latest/cluster/vms/user-guides/community/spark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2b187c5-4dbb-49e0-86a0-44b6d47dc44b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Config:\n",
    "- 2 Workers: 448 GB Memory, 24 Cores\n",
    "- 1 Driver: 224 GB Memory, 12 Cores\n",
    "Runtime:\n",
    "- 15.4.x-gpu-ml-scala2.12\n",
    "Type:\n",
    "- Standard_NC12s_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a642fbd-4ffa-4d4f-999e-402d045026cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Notes: \n",
    "\n",
    "- We recommend setting the argument num_cpus_worker_node to the number of CPU cores per Apache Spark worker node. Similarly, setting num_gpus_worker_node to the number of GPUs per Apache Spark worker node is optimal. With this configuration, each Apache Spark worker node launches one Ray worker node that will fully utilize the resources of each Apache Spark worker node.\n",
    "- Set the environment variable RAY_memory_monitor_refresh_ms to 0 within the Databricks cluster configuration when starting your Apache Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5935ee48-e128-4a5b-bf8c-b590d24291a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- In each spark worker node, we recommend making the sum of 'spark_executor_memory + num_Ray_worker_nodes_per_spark_worker * (memory_worker_node + object_store_memory_worker_node)' to be less than 'spark_worker_physical_memory * 0.8', otherwise it might lead to spark worker physical memory exhaustion and Ray task OOM errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460b42d6-8d54-4c49-bbcd-85f6dd3f0708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You configured 'spark.task.resource.gpu.amount' to 1.0,we recommend setting this value to 0 so that Spark jobs do not reserve GPU resources, preventing Ray-on-Spark workloads from having the maximum number of GPUs available. \n",
    "\n",
    "spark.conf.set(\"spark.task.resource.gpu.amount\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3952cc1-8850-46c1-b007-b5e3220d6a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "setup_ray_cluster(\n",
    "  max_worker_nodes=1,\n",
    "  num_cpus_per_node=12,\n",
    "  num_gpus_per_node=2,\n",
    "  num_cpus_head_node=12,\n",
    "  num_gpus_head_node=2,\n",
    "  collect_log_to_path=\"/dbfs/tmp/ws_ray_collected_logs\"\n",
    ")\n",
    "\n",
    "# Pass any custom Ray configuration with ray.init\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea251ec-58ff-48d7-8f9a-bbf28a2f628a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "import ray.train.torch\n",
    "\n",
    "def train_func():\n",
    "    # Model, Loss, Optimizer\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    # [1] Prepare model.\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    # model.to(\"cuda\")  # This is done by `prepare_model`\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Data\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
    "    train_data = FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    # [2] Prepare dataloader.\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(10):\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            # This is done by `prepare_data_loader`!\n",
    "            # images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # [3] Report metrics and checkpoint.\n",
    "        metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "             # Save model state_dict based on whether it's wrapped in DataParallel or not\n",
    "            if isinstance(model, torch.nn.DataParallel) or isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                torch.save(\n",
    "                    model.module.state_dict(),\n",
    "                    os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "                )\n",
    "            else:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "                )\n",
    "\n",
    "            ray.train.report(\n",
    "                metrics,\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10232bbe-11c8-4b30-af1e-ee2e00709060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from ray.train import RunConfig\n",
    "\n",
    "#  [4] Configure scaling and resource requirements.\n",
    "# Use GPU to allow cuda \n",
    "scaling_config = ray.train.ScalingConfig(num_workers=1, use_gpu=True)\n",
    "\n",
    "# Local path (/some/local/path/unique_run_name)\n",
    "run_config = RunConfig(storage_path=\"/dbfs/tmp/ray_ws_logs\", name=\"local\")\n",
    "\n",
    "# [5] Launch distributed training job.\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    # [5a] If running in a multi-node cluster, this is where you\n",
    "    # should configure the run's persistent storage that is accessible\n",
    "    # across all worker nodes.\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420af9b9-868f-4c8d-8b74-d5811cb0cce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.metrics)     # The metrics reported during training.\n",
    "display(result.checkpoint)  # The latest checkpoint reported during training.\n",
    "display(result.path)        # The path where logs are stored.\n",
    "display(result.error)       # The exception that was raised, if training failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8050191-76d1-40a7-b081-f413b5a2efd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# [6] Load the trained model.\n",
    "with result.checkpoint.as_directory() as checkpoint_dir:\n",
    "    model_state_dict = torch.load(os.path.join(checkpoint_dir, \"model.pt\"))\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121a50b5-3f93-4d13-a0af-0aae216e6aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray.util.spark.shutdown_ray_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0bc07d6-4c8a-4c4f-94bd-1e09abcc8756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "single_node_pytorch_ray",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
