{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc824ac6-e1f8-455f-9e75-38ff38a36ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install keras-hub==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212e920e-c893-4e55-8d53-7bd8276c16cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659d1536-4335-476d-9037-049b32ce307a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fea1eb9-adee-49f6-9022-b30c5bf2560d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Config:\n",
    "- 2 GPUs: A10\n",
    "- 1 Driver: 880 GB Memory, 72 Cores\n",
    "Runtime:\n",
    "- 15.4.x-gpu-ml-scala2.12\n",
    "Type:\n",
    "- Standard_NV72ads_A10_v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d56fb9-dc1f-4a66-be91-3218c8720ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "\n",
    "import keras_hub\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff22017f-b1ea-4d38-87db-a4ec1f0ea377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = f\"{os.getcwd()}/data\"\n",
    "cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4f1654-b6c1-41e9-ac0e-785e03ad2a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"dbfs:/Volumes/will_smith/datasets/wikitext/wikitext-103.zip\", f\"file:{cache_dir}/wikitext-103.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229fff96-d785-43f2-a834-8bbef4244255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download pretraining data.\n",
    "keras.utils.get_file(\n",
    "    cache_dir=cache_dir,\n",
    "    extract=True,\n",
    ")\n",
    "# wiki_dir = os.path.expanduser(f\"{cache_dir}/wikitext-103-raw/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f737fb-e488-4dee-915e-5a5d4e443d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download pretraining data.\n",
    "\n",
    "\n",
    "# Download finetuning data.\n",
    "keras.utils.get_file(\n",
    "    origin=\"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\",\n",
    "    cache_dir=cache_dir,\n",
    "    extract=True,\n",
    "    force_download=True,\n",
    ")\n",
    "\n",
    "sst_dir = os.path.expanduser(f\"{cache_dir}/SST-2/\")\n",
    "\n",
    "# Download vocabulary data.\n",
    "vocab_file = keras.utils.get_file(\n",
    "    cache_dir=cache_dir,\n",
    "    origin=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\",\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f231d26e-7b26-46f8-8b12-ac680d14a628",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameters"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing params.\n",
    "PRETRAINING_BATCH_SIZE = 128\n",
    "FINETUNING_BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 128\n",
    "MASK_RATE = 0.25\n",
    "PREDICTIONS_PER_SEQ = 32\n",
    "\n",
    "# Model params.\n",
    "NUM_LAYERS = 3\n",
    "MODEL_DIM = 256\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.1\n",
    "NORM_EPSILON = 1e-5\n",
    "\n",
    "# Training params.\n",
    "PRETRAINING_LEARNING_RATE = 5e-4\n",
    "PRETRAINING_EPOCHS = 8\n",
    "FINETUNING_LEARNING_RATE = 5e-5\n",
    "FINETUNING_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7360087b-2b49-48c2-9793-616ec93a7f60",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pipelines"
    }
   },
   "outputs": [],
   "source": [
    "# Load SST-2.\n",
    "sst_train_ds = tf.data.experimental.CsvDataset(\n",
    "    sst_dir + \"train.tsv\", [tf.string, tf.int32], header=True, field_delim=\"\\t\"\n",
    ").batch(FINETUNING_BATCH_SIZE)\n",
    "sst_val_ds = tf.data.experimental.CsvDataset(\n",
    "    sst_dir + \"dev.tsv\", [tf.string, tf.int32], header=True, field_delim=\"\\t\"\n",
    ").batch(FINETUNING_BATCH_SIZE)\n",
    "\n",
    "# Load wikitext-103 and filter out short lines.\n",
    "wiki_train_ds = (\n",
    "    tf.data.TextLineDataset(wiki_dir + \"wiki.train.raw\")\n",
    "    .filter(lambda x: tf.strings.length(x) > 100)\n",
    "    .batch(PRETRAINING_BATCH_SIZE)\n",
    ")\n",
    "wiki_val_ds = (\n",
    "    tf.data.TextLineDataset(wiki_dir + \"wiki.valid.raw\")\n",
    "    .filter(lambda x: tf.strings.length(x) > 100)\n",
    "    .batch(PRETRAINING_BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# Take a peak at the sst-2 dataset.\n",
    "print(sst_train_ds.unbatch().batch(4).take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a130b54-adf2-412c-b16b-b6719e4c5236",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Baseline"
    }
   },
   "outputs": [],
   "source": [
    "# This layer will turn our input sentence into a list of 1s and 0s the same size\n",
    "# our vocabulary, indicating whether a word is present in absent.\n",
    "multi_hot_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=4000, output_mode=\"multi_hot\"\n",
    ")\n",
    "multi_hot_layer.adapt(sst_train_ds.map(lambda x, y: x))\n",
    "multi_hot_ds = sst_train_ds.map(lambda x, y: (multi_hot_layer(x), y))\n",
    "multi_hot_val_ds = sst_val_ds.map(lambda x, y: (multi_hot_layer(x), y))\n",
    "\n",
    "# We then learn a linear regression over that layer, and that's our entire\n",
    "# baseline model!\n",
    "\n",
    "inputs = keras.Input(shape=(4000,), dtype=\"int32\")\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
    "baseline_model = keras.Model(inputs, outputs)\n",
    "baseline_model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "baseline_model.fit(multi_hot_ds, validation_data=multi_hot_val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f167946-ea99-49df-a465-943775f7d464",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PreTraining"
    }
   },
   "outputs": [],
   "source": [
    "# Setting sequence_length will trim or pad the token outputs to shape\n",
    "# (batch_size, SEQ_LENGTH).\n",
    "tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab_file,\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    ")\n",
    "# Setting mask_selection_length will trim or pad the mask outputs to shape\n",
    "# (batch_size, PREDICTIONS_PER_SEQ).\n",
    "masker = keras_hub.layers.MaskedLMMaskGenerator(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    mask_selection_rate=MASK_RATE,\n",
    "    mask_selection_length=PREDICTIONS_PER_SEQ,\n",
    "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    inputs = tokenizer(inputs)\n",
    "    outputs = masker(inputs)\n",
    "    # Split the masking layer outputs into a (features, labels, and weights)\n",
    "    # tuple that we can use with keras.Model.fit().\n",
    "    features = {\n",
    "        \"token_ids\": outputs[\"token_ids\"],\n",
    "        \"mask_positions\": outputs[\"mask_positions\"],\n",
    "    }\n",
    "    labels = outputs[\"mask_ids\"]\n",
    "    weights = outputs[\"mask_weights\"]\n",
    "    return features, labels, weights\n",
    "\n",
    "\n",
    "# We use prefetch() to pre-compute preprocessed batches on the fly on the CPU.\n",
    "pretrain_ds = wiki_train_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "pretrain_val_ds = wiki_val_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Preview a single input example.\n",
    "# The masks will change each time you run the cell.\n",
    "print(pretrain_val_ds.take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "285643ca-f551-439a-b7a1-e28d82d168e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=\"int32\")\n",
    "\n",
    "# Embed our tokens with a positional embedding.\n",
    "embedding_layer = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    embedding_dim=MODEL_DIM,\n",
    ")\n",
    "outputs = embedding_layer(inputs)\n",
    "\n",
    "# Apply layer normalization and dropout to the embedding.\n",
    "outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n",
    "outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)\n",
    "\n",
    "# Add a number of encoder blocks\n",
    "for i in range(NUM_LAYERS):\n",
    "    outputs = keras_hub.layers.TransformerEncoder(\n",
    "        intermediate_dim=INTERMEDIATE_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        layer_norm_epsilon=NORM_EPSILON,\n",
    "    )(outputs)\n",
    "\n",
    "encoder_model = keras.Model(inputs, outputs)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "788fcf9a-28da-438b-8f03-e20302fc7d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the pretraining model by attaching a masked language model head.\n",
    "inputs = {\n",
    "    \"token_ids\": keras.Input(shape=(SEQ_LENGTH,), dtype=\"int32\", name=\"token_ids\"),\n",
    "    \"mask_positions\": keras.Input(\n",
    "        shape=(PREDICTIONS_PER_SEQ,), dtype=\"int32\", name=\"mask_positions\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Encode the tokens.\n",
    "encoded_tokens = encoder_model(inputs[\"token_ids\"])\n",
    "\n",
    "# Predict an output word for each masked input token.\n",
    "# We use the input token embedding to project from our encoded vectors to\n",
    "# vocabulary logits, which has been shown to improve training efficiency.\n",
    "outputs = keras_hub.layers.MaskedLMHead(\n",
    "    token_embedding=embedding_layer.token_embedding,\n",
    "    activation=\"softmax\",\n",
    ")(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n",
    "\n",
    "# Define and compile our pretraining model.\n",
    "pretraining_model = keras.Model(inputs, outputs)\n",
    "pretraining_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(PRETRAINING_LEARNING_RATE),\n",
    "    weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
    "    jit_compile=True,\n",
    ")\n",
    "\n",
    "# Pretrain the model on our wiki text dataset.\n",
    "pretraining_model.fit(\n",
    "    pretrain_ds,\n",
    "    validation_data=pretrain_val_ds,\n",
    "    epochs=PRETRAINING_EPOCHS,\n",
    ")\n",
    "\n",
    "# Save this base model for further finetuning.\n",
    "encoder_model.save(\"encoder_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ba5b21c-e4c0-4352-9b20-163283fa7762",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fine-tune"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(sentences, labels):\n",
    "    return tokenizer(sentences), labels\n",
    "\n",
    "\n",
    "# We use prefetch() to pre-compute preprocessed batches on the fly on our CPU.\n",
    "finetune_ds = sst_train_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "finetune_val_ds = sst_val_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Preview a single input example.\n",
    "print(finetune_val_ds.take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f66837-e318-4d52-a89e-13205dd05223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reload the encoder model from disk so we can restart fine-tuning from scratch.\n",
    "encoder_model = keras.models.load_model(\"encoder_model.keras\", compile=False)\n",
    "\n",
    "# Take as input the tokenized input.\n",
    "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=\"int32\")\n",
    "\n",
    "# Encode and pool the tokens.\n",
    "encoded_tokens = encoder_model(inputs)\n",
    "pooled_tokens = keras.layers.GlobalAveragePooling1D()(encoded_tokens[0])\n",
    "\n",
    "# Predict an output label.\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(pooled_tokens)\n",
    "\n",
    "# Define and compile our fine-tuning model.\n",
    "finetuning_model = keras.Model(inputs, outputs)\n",
    "finetuning_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(FINETUNING_LEARNING_RATE),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Finetune the model for the SST-2 task.\n",
    "finetuning_model.fit(\n",
    "    finetune_ds,\n",
    "    validation_data=finetune_val_ds,\n",
    "    epochs=FINETUNING_EPOCHS,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1412556571894614,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "single_node_tensorflow_ray",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
